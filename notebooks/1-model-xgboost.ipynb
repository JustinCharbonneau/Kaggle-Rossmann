{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#3366BB'>XGBoost (eXtreme Gradient Boosting)</font>\n",
    "Author: Justin Charbonneau\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- [Article Summary](#article-sum)\n",
    "- [Experiment 001 (Default)](#exp001)\n",
    "    - [Different Tree Methods](#tree_method)\n",
    "    - [Train Model](#train001)\n",
    "- [Experiment 002 (HyperOpt)](#exp002)\n",
    "    - [Experiment Setup](#experiment-setup)\n",
    "    - [Hyperparameter Tuning](#hyperopt)\n",
    "    - [Train Model](#train002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Summary\n",
    "## <span style=\"color:#3366BB\">[[Article](https://arxiv.org/pdf/1603.02754.pdf)]  [[Code](https://github.com/dmlc/xgboost)]</span>\n",
    "\n",
    "**Abstract**\n",
    ">Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end to end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.\n",
    "\n",
    "**Contributions**\n",
    "\n",
    "- Design and build a highly scalable end-to-end tree boosting system\n",
    "- Propose a theoretically justified weighted quantile sketch for efficient proposal calculation\n",
    "- Introduce a novel sparsity-aware algorithm for parallel tree learning\n",
    "- Propose an effective cache-aware block structure for out-of-core tree learning\n",
    "- regularized learning objective\n",
    "\n",
    "**Few points on XGBoost**\n",
    "\n",
    "- Scalable to millions of rows\n",
    "- Proven Performance with hyperparameter tunning\n",
    "- Very Fast on GPU\n",
    "- Simple code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Results\n",
    "\n",
    "Private score and puplic score were retreived after submitting the predictions to Kaggle.\n",
    "\n",
    "| Experiment ID | Categorical Variables | NaN-cats | NaN-cont | Target Transformation | Hyperparameter Search | Backtesting            | Private Score | Public Score\n",
    "|---------------|-----------------------|----------|----------|-----------------------|-----------------------|------------------------|---------------|---------------\n",
    "| 001           | Target encoder        | XGBoost  | XGBoost  | Log transform         | Default               | No                     | 0.16925       | 0.17975\n",
    "| 002           | Target encoder        | XGBoost  | XGBoost  | Log transform         | HyperOpt (100)        | TimeSeriesSplit k = 3  | 0.13975       | 0.12481\n",
    "| 003           | Entity Embeddings     | #NAN#    | FastAI   | Log transform         | Default               | No                     | 0.15251       | 0.14079\n",
    "| 004           | Entity Embeddings     | #NAN#    | FastAI   | Log transform         | HyperOpt (100)        | TimeSeriesSplit k = 3  | 0.13081       | 0.11572\n",
    "\n",
    "***\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import xgboost as xgb\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Hyperparameter search\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, plotting\n",
    "\n",
    "# Utils\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For reproducibility\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Exploratory Data Analysis\n",
    "\n",
    "The data has already been processed from the fastAI course. You can checkout the data-proprocessing notebook for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_parquet('../data/03_primary/clean_train_valid.parquet')\n",
    "print(f'There are {data.shape[0]} rows and {data.shape[1]} columns')\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features defined by fastai tutorial\n",
    "cat_vars = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "    'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "    'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "    'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
    "\n",
    "cont_vars = ['CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "   'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "   'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "   'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday']\n",
    "\n",
    "dep_var = 'Sales'\n",
    "\n",
    "# Filtrer les colonnes\n",
    "data = data[cat_vars + cont_vars + [dep_var, 'Date']]\n",
    "\n",
    "print(f'We now have {data.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the datatypes and if the columns have NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([data.dtypes,data.isna().sum()],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the column 'StoreType' has the data type 'object'.\n",
    "\n",
    "Let's see the type of the first instance of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data.StoreType.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost doesn't like categorical data, contrary to CatBoost and LGBM.\n",
    "\n",
    "Here's the error you would get if you ignored this fact and tried to train a model ... (Yes, I initially tried lol)\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**ValueError:** DataFrame.dtypes for data must be int, float or bool.  \n",
    "Did not expect the data types in fields StoreType, Assortment, PromoInterval, State, Events, Date\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most 'categoriacle' variables (defined by the fastai course) are already numeric, we will simply focus on the 5 others (StoreType, Assortment, PromoInterval, State, Events). The date will be used for splitting time-wise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: Can we treat this problem as a time series problem and split the data in time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_parquet('../data/03_primary/clean_internal_test.parquet')\n",
    "\n",
    "print(f\"The number of distinct stores in the testing data: {len(data_test['Store'].unique())}\")\n",
    "print(f\"The number of distinct stores in the training data: {len(data['Store'].unique())}\")\n",
    "\n",
    "data[['Store','Date']].groupby('Date').count().plot(style='.')\n",
    "plt.title('Number of unique stores per day in the training data')\n",
    "plt.show()\n",
    "\n",
    "data_test[['Store','Date']].groupby('Date').count().plot(style='.')\n",
    "plt.title('Number of unique stores per day in the testing data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "I defined a function for ease of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(fpath, fname_train, fname_test, seed):\n",
    "    \"\"\"\n",
    "    Load data and preprocess it for training a model. This will also fill \n",
    "    missing values for continuous variables with the median and create a \n",
    "    flag column correspondingly. This isn't used for hyper-parameter tunning.\n",
    "    \n",
    "    Args\n",
    "        fpath: string, folder path \n",
    "        fname_train: string, name with parquet extention\n",
    "        fname_test: string, name with pkl extention\n",
    "        seed: int, \n",
    "    Return\n",
    "        dtrain: xgb.DMatrix, dataset used for training\n",
    "        dvalid: xgb.DMatrix, dataset used for early stopping\n",
    "        dtest: xgb.DMatrix, dataset used for submittion to kaggle\n",
    "    \"\"\"\n",
    "    # Define the features to load\n",
    "    columns = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', \n",
    "                'CompetitionMonthsOpen', 'Promo2Weeks', 'StoreType', 'Assortment', \n",
    "                'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', \n",
    "                'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', \n",
    "                'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw', 'CompetitionDistance', \n",
    "                'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', 'Max_Humidity', \n",
    "                'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', \n",
    "                'CloudCover', 'trend', 'trend_DE', 'AfterStateHoliday', 'BeforeStateHoliday', \n",
    "                'Promo', 'SchoolHoliday', 'Date', 'Sales']\n",
    "    \n",
    "    # 1) Load data\n",
    "    train = pd.read_parquet(Path(fpath,fname_train), columns=columns)\n",
    "    test = pd.read_pickle(Path(fpath,fname_test))\n",
    "    columns.remove('Sales')\n",
    "    test = test[columns + ['Id']]\n",
    "    \n",
    "    # 2) Let's use the date as the index and sort the data\n",
    "    train.sort_values('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.sort_values('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "    columns.remove('Date')\n",
    "    test_ids = test.pop('Id') #Useful for submission\n",
    "\n",
    "    # 3) Deal with missing continuous values\n",
    "    for col_name in ['CompetitionDistance', 'CloudCover']:\n",
    "        # Add na cols\n",
    "        train[col_name+'_na'] = pd.isnull(train[col_name])\n",
    "        test[col_name+'_na'] = pd.isnull(test[col_name])\n",
    "        # Fill missing with median (default in FastAI)\n",
    "        fillter = train[col_name].median()\n",
    "        train[col_name] =  train[col_name].fillna(fillter)\n",
    "        test[col_name] =  test[col_name].fillna(fillter)\n",
    "        columns.append(col_name+'_na')\n",
    "\n",
    "    # 4) Apply log transform to the target variable\n",
    "    train['Sales'] = np.log1p(train['Sales'])\n",
    "\n",
    "    # 5) Set asside a random 1% sample for early stopping\n",
    "    # I'm separating the X and y simply for ease of use for the target encoder\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train[columns], train['Sales'], test_size=0.01, random_state=seed)\n",
    "\n",
    "    # 6) Deal with categorical variables\n",
    "    te = TargetEncoder(handle_missing='value')\n",
    "    train_X = te.fit_transform(train_X, cols=['StoreType', 'Assortment', 'PromoInterval', 'State', 'Events'], y=train_y)\n",
    "    valid_X = te.transform(valid_X)\n",
    "    test = te.transform(test)\n",
    "\n",
    "    # 7) Convert to DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(train_X, train_y)\n",
    "    dvalid = xgb.DMatrix(valid_X, valid_y)\n",
    "    dtest = xgb.DMatrix(test)\n",
    "    \n",
    "    return dtrain, dvalid, dtest, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric:\n",
    "# src: https://www.kaggle.com/c/rossmann-store-sales/discussion/16794 (Chenglong Chen)\n",
    "\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 001 - Default Hyperparameters\n",
    "\n",
    "- `eta`: 0.3, alias: learning rate\n",
    "- `gamma`: 0, alias: min_split_loss\n",
    "- `max_depth`: 6\n",
    "- `min_child_weight`: 1\n",
    "- `max_delta_step`: 0\n",
    "- `subsample`: 1\n",
    "- `lambda` 1, alias: reg_lambda\n",
    "- `alpha` 0, alias: reg_alpha\n",
    "- `num_boost_round`: 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepare the data\n",
    "dtrain, dvalid, dtest, test_ids = data_pipeline('../data/03_primary', 'clean_train_valid.parquet', 'test_clean.pkl', seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Training\n",
    "num_boost_round = 100\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, let's look at the training time for different tree methods for just a few iterations. \n",
    "- auto\n",
    "- hist\n",
    "- gpu_hist (saving best for last... obviously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'objective':'reg:squarederror', 'tree_method':'auto'}\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, feval=rmspe_xg, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'objective':'reg:squarederror','tree_method':'hist'}\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, feval=rmspe_xg, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "params = {'objective':'reg:squarederror','tree_method':'gpu_hist'}\n",
    "gbm = xgb.train(params, dtrain, num_boost_round, evals=watchlist, feval=rmspe_xg, verbose_eval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a great different in training time between the three methods. This is crucial to know, especially when we want to do hyperparameter tunning. For now on, I think we can aggree to use gpu_hist.\n",
    "\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_curves_001 = {}\n",
    "params_001 = {'objective':'reg:squarederror', 'tree_method':'gpu_hist'}\n",
    "\n",
    "model_001 = xgb.train(params=params_001, \n",
    "                      dtrain=dtrain, \n",
    "                      early_stopping_rounds=20, \n",
    "                      num_boost_round=4000, \n",
    "                      feval=rmspe_xg, \n",
    "                      verbose_eval=False, \n",
    "                      evals=watchlist, \n",
    "                      evals_result=training_curves_001)\n",
    "\n",
    "predictions_001 = model_001.predict(dtest)\n",
    "\n",
    "pd.DataFrame({'Id':test_ids,\n",
    "              'Sales':np.exp(predictions_001)}).to_csv('../data/03_primary/exp_001.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(training_curves_001['train']['rmspe'], label='Train')\n",
    "plt.plot(training_curves_001['eval']['rmspe'], label='Eval')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSPE')\n",
    "plt.title('RMSPE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(training_curves_001['train']['rmspe'][:100], label='Train')\n",
    "plt.plot(training_curves_001['eval']['rmspe'][:100], label='Eval')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSPE')\n",
    "plt.title('RMSPE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second plot was illustrated to show that around the 18th iteration, the RMSPE doesn't decrease. Depending on the learning rate used, this would plateau for more iterations which would trigger the early stopping. This is why sometimes I uses 20 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Results\n",
    "\n",
    "Private Score 0.16925  \n",
    "Public Score 0.17975\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2 - Hyperparameter Search with Hyperopt\n",
    "\n",
    "I start by defining a uniform distribution to the hyperparameters, and the it will update the posterior distribution after each iteration to sample better hyperparameter.\n",
    "\n",
    "- `eta`:\n",
    "- `max_depth`:\n",
    "- `min_child_weight`:\n",
    "- `subsample`:\n",
    "- `gamma`: -> Ne fera pas partie car ici ca tout fourer.\n",
    "- `colsample_bytree`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology\n",
    "\n",
    "The prediction period for the test set is 42 days and the training period is 899 days.  \n",
    "To do cross-validation with a test set that lasts about the same length, we need to do 20 splits.  \n",
    "However, this would take forever to do our hyperparameter search. Therefore, I suggest to split into 20, but skip the first iterations.  \n",
    "\n",
    "See bellow for an illustration:  \n",
    "\n",
    "![image](https://user-images.githubusercontent.com/25487881/78314966-a32d8600-7529-11ea-9560-b80d5c1e5435.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperopt needs two functions. \n",
    "1) It needs an optimize function which defines the hyperparameter space  \n",
    "2) It needs a scoring function that calculates the score or RMSPE in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize():\n",
    "    space = {\n",
    "            # Learning rate: default 0.3 -> range: [0,3]\n",
    "           'eta': hp.quniform('eta', 0.01, 0.3, 0.001),\n",
    "            # Control complexity (control overfitting)\n",
    "            # Maximum depth of a tree: default 6 -> range: [0:∞]\n",
    "            'max_depth':  hp.choice('max_depth', np.arange(5, 10, dtype=int)),\n",
    "            # Minimum sum of instance weight (hessian) needed in a child: default 1\n",
    "            'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "            # Minimum loss reduction required: default 0 -> range: [0,∞]\n",
    "            'gamma': hp.quniform('gamma', 0, 5, 0.5),\n",
    "\n",
    "            # Add randomness to make training robust to noise (control overfitting)\n",
    "            # Subsample ratio of the training instance: default 1\n",
    "            'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "            # Subsample ratio of columns when constructing each tree: default 1\n",
    "            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "            \n",
    "            # Regression problem\n",
    "            'objective': 'reg:squarederror',\n",
    "            # For reproducibility\n",
    "            'seed': seed,\n",
    "            # Faster computation\n",
    "            'tree_method':'gpu_hist'\n",
    "            }\n",
    "        \n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=100)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't use data pipeline as it changes a lot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features to load\n",
    "columns = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', \n",
    "            'CompetitionMonthsOpen', 'Promo2Weeks', 'StoreType', 'Assortment', \n",
    "            'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', \n",
    "            'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', \n",
    "            'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw', 'CompetitionDistance', \n",
    "            'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', 'Max_Humidity', \n",
    "            'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', \n",
    "            'CloudCover', 'trend', 'trend_DE', 'AfterStateHoliday', 'BeforeStateHoliday', \n",
    "            'Promo', 'SchoolHoliday', 'Date', 'Sales']\n",
    "\n",
    "# 1) Load data\n",
    "tunning_dataset = pd.read_parquet(Path('../data/03_primary', 'clean_train_valid.parquet'), columns=columns)\n",
    "#columns.remove('Sales')\n",
    "\n",
    "# 2) Let's use the date as the index and sort the data\n",
    "tunning_dataset.sort_values('Date', inplace=True)\n",
    "tunning_dataset.set_index('Date', inplace=True)\n",
    "columns.remove('Date')\n",
    "\n",
    "tunning_dataset_X = tunning_dataset\n",
    "tunning_dataset_y = tunning_dataset_X.pop('Sales')\n",
    "\n",
    "# 3) Apply log transform\n",
    "tunning_dataset_y = np.log1p(tunning_dataset_y)\n",
    "\n",
    "# Number of cross-validation folds (from the last)\n",
    "pred_folds = 3\n",
    "train_times = []\n",
    "def score(params):\n",
    "    \"\"\"\n",
    "    Calculate the score for the desired number of folds. Uses early stopping.\n",
    "    In this function, we apply the log transform to the sales.\n",
    "    \n",
    "    Args\n",
    "        params: dict\n",
    "    Returns\n",
    "        score: float\n",
    "    \"\"\"\n",
    "    # Initialize variables: timer, scoring list and number of splits with a counter, Number of iteration\n",
    "    start_time = time()\n",
    "    score_list = [] \n",
    "    tscv = TimeSeriesSplit(n_splits=20)\n",
    "    split_iteration = -1\n",
    "    \n",
    "    for train_index, test_index in tscv.split(tunning_dataset_X):\n",
    "        \n",
    "        # Select the folds from the end (superintended) for the desired number of splits.\n",
    "        split_iteration+=1\n",
    "        if split_iteration < 20 - pred_folds: continue\n",
    "        \n",
    "        # 4) Select 1% of the training data for early stopping\n",
    "        train_index, es_index = train_test_split(train_index, test_size=0.01, random_state=seed)\n",
    "        \n",
    "        # Select data by index from the time series cross validatation split\n",
    "        X_train, X_val, X_test  = tunning_dataset_X.iloc[train_index].copy(), tunning_dataset_X.iloc[es_index].copy(), tunning_dataset_X.iloc[test_index].copy()\n",
    "        y_train, y_val, y_test  = tunning_dataset_y.iloc[train_index].copy(), tunning_dataset_y.iloc[es_index].copy(), tunning_dataset_y.iloc[test_index].copy()\n",
    "        \n",
    "        # 5) Deal with missing continuous values\n",
    "        for col_name in ['CompetitionDistance', 'CloudCover']:\n",
    "            # Add na cols\n",
    "            X_train[col_name+'_na'] = pd.isnull(X_train[col_name])\n",
    "            X_val[col_name+'_na'] = pd.isnull(X_val[col_name])\n",
    "            X_test[col_name+'_na'] = pd.isnull(X_test[col_name])\n",
    "            # Fill missing with median (default in FastAI)\n",
    "            fillter = X_train[col_name].median()\n",
    "            X_train[col_name] =  X_train[col_name].fillna(fillter)\n",
    "            X_val[col_name] =  X_val[col_name].fillna(fillter)\n",
    "            X_test[col_name] =  X_test[col_name].fillna(fillter)\n",
    "        \n",
    "        # 6) Deal with categorical variables\n",
    "        te = TargetEncoder(handle_missing='value')\n",
    "        X_train = te.fit_transform(X_train, cols=['StoreType', 'Assortment', 'PromoInterval', 'State', 'Events'], y=y_train)\n",
    "        X_val = te.transform(X_val)\n",
    "        X_test = te.transform(X_test)\n",
    "        \n",
    "        # 7) Convert to DMatrix for XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        dvalid = xgb.DMatrix(X_val, y_val)\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        \n",
    "        # The second from the list will be used by xgboost for early stopping -> dvalid\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "            \n",
    "        # Can use feval for a custom objective function\n",
    "        model = xgb.train(params, dtrain, early_stopping_rounds=100, \\\n",
    "                          num_boost_round=4000, verbose_eval=False, feval=rmspe_xg, evals=watchlist) \n",
    "\n",
    "        # validation - this will be the score that we append to a list. which will be fed as the score? Is all of this the score?\n",
    "        y_pred = model.predict(xgb.DMatrix(X_test))\n",
    "        error = rmspe(y_test, y_pred)\n",
    "        score_list.append(error)\n",
    "\n",
    "    #print(f'Took  {np.round(time()-start_time, 0)} (s) - RMSPE score: {np.mean(score_list)} ')\n",
    "    \n",
    "    train_times.append(np.round(time()-start_time, 0))\n",
    "    \n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# trials will contain logging information\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = optimize()\n",
    "print(f'The best hyperparameters are: {best_hyperparams}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trials as a pandas dataframe\n",
    "summary_table = pd.DataFrame()\n",
    "\n",
    "for i in range(len(trials.trials)-1):\n",
    "    row = pd.concat([pd.DataFrame({'loss':[trials.trials[i]['result']['loss']]}), \\\n",
    "                     pd.DataFrame(trials.trials[i]['misc']['vals'])], axis=1)\n",
    "    summary_table = summary_table.append(row)\n",
    "\n",
    "summary_table = pd.concat([pd.DataFrame({'exp_time':train_times}),summary_table.reset_index(drop=True)],axis=1)\n",
    "summary_table = summary_table.sort_values('loss')\n",
    "summary_table.to_pickle('trials_002.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperopt has plotting functions that work directly with the trials, \n",
    "# but they are pretty horrible: plotting.main_plot_vars, plotting.main_plot_history\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2,ncols=3,figsize=(16,8))\n",
    "\n",
    "names = ['min_child_weight','eta','gamma','subsample','max_depth','colsample_bytree']\n",
    "id_names = 0\n",
    "for row in range(2):\n",
    "    for col in range(3):\n",
    "        axes[row,col].scatter(x=summary_table[names[id_names]], y=summary_table['loss'], alpha=0.4)\n",
    "        axes[row,col].set_xlabel(names[id_names])\n",
    "        axes[row,col].set_ylabel('loss')\n",
    "        id_names += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparams['max_depth'] = 8 # hp.choice will return the index of the choices.\n",
    "best_hyperparams['objective'] = 'reg:squarederror'\n",
    "best_hyperparams['tree_method'] = 'gpu_hist'\n",
    "\n",
    "training_curves_002 = {}\n",
    "model_exp002 = xgb.train(params=best_hyperparams, \n",
    "                         dtrain=dtrain, \n",
    "                         num_boost_round=4000, \n",
    "                         early_stopping_rounds=100, \n",
    "                         feval=rmspe_xg, \n",
    "                         verbose_eval=500, \n",
    "                         evals=watchlist, \n",
    "                         evals_result=training_curves_002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_002 = model_exp002.predict(dtest)\n",
    "\n",
    "pd.DataFrame({'Id':test_ids,\n",
    "              'Sales':np.exp(predictions_002)}).to_csv('../data/03_primary/exp_002.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(training_curves_002['train']['rmspe'],label='Train')\n",
    "plt.plot(training_curves_002['eval']['rmspe'],label='Eval')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSPE')\n",
    "plt.title('RMSPE Loss without early stopping')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Results\n",
    "    \n",
    "Private Score: 0.13975  \n",
    "Public Score: 0.12481\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fun\n",
    "xgb.plot_importance(model_exp002)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the variable 'Store' is the most important feature according to the plot. Hence, if we learn a better representation for the different stores, we should get a gain in performance.\n",
    "\n",
    "Checkout the XGBoost with Entity Embeddings for learning better representations of categorical variables like 'Store'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:supertab-xg] *",
   "language": "python",
   "name": "conda-env-supertab-xg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
