{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='#3366BB'>XGBoost with Entity Embeddings</font>\n",
    "\n",
    "## <span style=\"color:#3366BB\">[[Article](https://arxiv.org/pdf/1604.06737.pdf)]</span>\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- [Train Entity Embeddings (PyTorch)](#entity-embeddings)\n",
    "- [Experiment 003 (Default)](#exp003)\n",
    "    - [Train Model](#train003)\n",
    "- [Experiment 004 (HyperOpt)](#exp004)\n",
    "    - [Hyperparameter Tuning](#hyper)\n",
    "    - [Train Model](#train004)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Results\n",
    "\n",
    "| Experiment ID | Categorical Variables | NaN-cats | NaN-cont | Target Transformation | Hyperparameter Search | Backtesting            | Private Score | Public Score\n",
    "|---------------|-----------------------|----------|----------|-----------------------|-----------------------|------------------------|---------------|---------------\n",
    "| 001           | Target encoder        | XGBoost  | XGBoost  | Log transform         | Default               | No                     | 0.16925       | 0.17975\n",
    "| 002           | Target encoder        | XGBoost  | XGBoost  | Log transform         | HyperOpt (100)        | TimeSeriesSplit k = 3  | 0.13975       | 0.12481\n",
    "| 003           | Entity Embeddings     | #NAN#    | FastAI   | Log transform         | Default               | No                     | 0.15251       | 0.14079\n",
    "| 004           | Entity Embeddings     | #NAN#    | FastAI   | Log transform         | HyperOpt (100)        | TimeSeriesSplit k = 3  | 0.13081       | 0.11572\n",
    "\n",
    "***\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "import xgboost as xgb\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Hyperparameter search\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, space_eval\n",
    "\n",
    "# Utils\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Entity Embeddings\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric:\n",
    "# src: https://www.kaggle.com/c/rossmann-store-sales/discussion/16794 (Chenglong Chen)\n",
    "\n",
    "def ToWeight(y):\n",
    "    w = np.zeros(y.shape, dtype=float)\n",
    "    ind = y != 0\n",
    "    w[ind] = 1./(y[ind]**2)\n",
    "    return w\n",
    "\n",
    "def rmspe(yhat, y):\n",
    "    y = np.exp(y) - 1       \n",
    "    yhat = np.exp(yhat) - 1 \n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n",
    "    return rmspe\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = y.get_label()\n",
    "    y = np.exp(y) - 1\n",
    "    yhat = np.exp(yhat) - 1\n",
    "    w = ToWeight(y)\n",
    "    rmspe = np.sqrt(np.mean(w * (y - yhat)**2))\n",
    "    return \"rmspe\", rmspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Embeddings\n",
    "\n",
    "For this section, I will use all categorical variables defined in the fastAI course. I will train embeddings for each categorical variables and use them in an XGBoost model. For the missing categorical variables, a new category names #NAN# will be used. Hence, an embedding will be learned for those.  \n",
    "\n",
    "I tried applying both BN and Dropout before or after the relu activation, and it seemed that after resulted in better performance for default parameters.\n",
    "\n",
    "For this part, I relied on this source: https://yashuseth.blog/2018/07/22/pytorch-neural-network-for-tabular-data-with-categorical-embeddings/  \n",
    "\n",
    "*Figure 1: Example architecture for two categories:*\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/25487881/78181963-42bc1d00-7433-11ea-8236-6dd6f64e247a.png)\n",
    "\n",
    "Now we will define the dataset for the dataloader and define the Pytorch model for learning the entity embeddings. Usually online you will find sources that integrade models for end-to-end training and allowing both categorical and continuous variables. In this case, the model is only ment to work with categorical variables and purely for learning the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalDataset(Dataset):\n",
    "    def __init__(self, data, output_col=None):\n",
    "        \"\"\"\n",
    "        Characterizes a Dataset for PyTorch\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        data: pandas data frame\n",
    "          The data frame object for the input data. It must\n",
    "          contain all the continuous, categorical and the\n",
    "          output columns to be used.\n",
    "\n",
    "        output_col: string\n",
    "          The name of the output variable column in the data\n",
    "          provided.\n",
    "        \"\"\"\n",
    "\n",
    "        # Shape of the full dataset\n",
    "        self.n = data.shape[0]\n",
    "\n",
    "        # Store in 'y' the target values of the full dataset\n",
    "        self.y = data[output_col].astype(np.float32).values.reshape(-1, 1)\n",
    "        \n",
    "        # Store list of categorical columns\n",
    "        self.cat_cols = [col for col in data.columns if col != output_col]\n",
    "            \n",
    "        # Ensure the datatypes of the categorical variables are of int64 and in a numpy.ndarray\n",
    "        self.cat_X = data[self.cat_cols].astype(np.int64).values\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Denotes the total number of samples.\n",
    "        \"\"\"\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generates one sample of data based on the index.\n",
    "        \"\"\"\n",
    "        return [self.y[index], self.cat_X[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "\n",
    "    def __init__(self, emb_dims, lin_layer_sizes,\n",
    "               output_size, emb_dropout, lin_layer_dropouts):\n",
    "\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        emb_dims: List of two element tuples\n",
    "          This list will contain a two element tuple for each\n",
    "          categorical feature. The first element of a tuple will\n",
    "          denote the number of unique values of the categorical\n",
    "          feature. The second element will denote the embedding\n",
    "          dimension to be used for that feature.\n",
    "\n",
    "        lin_layer_sizes: List of integers.\n",
    "          The size of each linear layer. The length will be equal\n",
    "          to the total number\n",
    "          of linear layers in the network.\n",
    "\n",
    "        output_size: Integer\n",
    "          The size of the final output.\n",
    "\n",
    "        emb_dropout: Float\n",
    "          The dropout to be used after the embedding layers.\n",
    "\n",
    "        lin_layer_dropouts: List of floats\n",
    "          The dropouts to be used after each linear layer.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        # We create a list of embedding layers (one for each categorical feature)\n",
    "        # An embedding layer takes in the size of the vocabulary and the number of dimensions\n",
    "        # nn.Embedding = A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        self.emb_layers = nn.ModuleList([nn.Embedding(dict_size, emb_dim) for dict_size, emb_dim in emb_dims])\n",
    "\n",
    "        # Need the concatenated length of the embeddings\n",
    "        self.sum_length_embeddings = sum([emb_dim for dict_size, emb_dim in emb_dims])\n",
    "\n",
    "        # The first linear layer (input dimensions , output dimensions): dense layer 0 in the EE paper\n",
    "        first_lin_layer = nn.Linear(self.sum_length_embeddings, lin_layer_sizes[0])\n",
    "        \n",
    "        # Create alyers than contains first linear layer and adds a second layer: dense layer 1 in the EE paper\n",
    "        self.lin_layers = nn.ModuleList([first_lin_layer] + [nn.Linear(lin_layer_sizes[0], lin_layer_sizes[1])])\n",
    "        \n",
    "        # Initialize weights in the two linear layers\n",
    "        for lin_layer in self.lin_layers:\n",
    "            nn.init.kaiming_normal_(lin_layer.weight.data)\n",
    "\n",
    "        # Output Layer\n",
    "        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n",
    "        nn.init.kaiming_normal_(self.output_layer.weight.data)\n",
    "        \n",
    "        # Batch Norm Layers - after all linear layers in network\n",
    "        self.bn_layers = nn.ModuleList([nn.BatchNorm1d(size) for size in lin_layer_sizes])\n",
    "\n",
    "        # Dropout Layers. They are separated due to different dropout probabilities\n",
    "        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n",
    "        self.droput_layers = nn.ModuleList([nn.Dropout(size) for size in lin_layer_dropouts])\n",
    "\n",
    "    def forward(self, cat_data):\n",
    "\n",
    "        # Select appropriate lookup table for each id of data for each column\n",
    "        x = [emb_layer(cat_data[:, i]) for i,emb_layer in enumerate(self.emb_layers)]\n",
    "        \n",
    "        # Here we concat (flatten) the embeddings selected columns wise\n",
    "        x = torch.cat(x, 1)\n",
    "        \n",
    "        # Apply the embedding dropout\n",
    "        x = self.emb_dropout_layer(x)\n",
    "\n",
    "        for lin_layer, dropout_layer, bn_layer in zip(self.lin_layers, self.droput_layers, self.bn_layers):\n",
    "\n",
    "            x = F.relu(lin_layer(x))\n",
    "            x = bn_layer(x)\n",
    "            x = dropout_layer(x)\n",
    "            \n",
    "#             # FastiAI approach: apply BN and dropout before relu\n",
    "#             x = bn_layer(lin_layer(x))\n",
    "#             x = dropout_layer(x)\n",
    "#             x = F.relu(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which features to use - see lesson 6 of fastai.\n",
    "categorical_features = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "                        'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "                        'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "                        'SchoolHoliday_fw', 'SchoolHoliday_bw']\n",
    "output_feature = ['Sales']\n",
    "\n",
    "# Load data\n",
    "data_ee = pd.read_parquet('../data/03_primary/clean_train_valid.parquet')\n",
    "\n",
    "# Filter columns \n",
    "data_ee = data_ee[categorical_features + output_feature + ['Date']]\n",
    "data_ee.sort_values('Date', inplace=True)\n",
    "data_ee.set_index('Date', inplace=True)\n",
    "\n",
    "# Deal with missing categorical values\n",
    "data_ee['PromoInterval'] = data_ee['PromoInterval'].fillna('#NAN#')\n",
    "data_ee['Events'] = data_ee['Events'].fillna('#NAN#')\n",
    "\n",
    "# The cross-validation (val) sets will be made starting from row 688500. Thus we can use the data before to learn the embeddings\n",
    "data_ee = data_ee.iloc[:688500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is almost ready. All that's left is use the label encoding technique to replace the categorical variables with integers. These will be used in the lookup table of the embedding layers of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictionary that will contain all different label encoders by columns\n",
    "label_encoders = {}\n",
    "# Loop over categorical features, initialize a new label encoder and fit_transform the data\n",
    "for cat_col in categorical_features:\n",
    "    label_encoders[cat_col] = LabelEncoder()\n",
    "    data_ee[cat_col] = label_encoders[cat_col].fit_transform(data_ee[cat_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a proper dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CategoricalDataset(data=data_ee, output_col=output_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a heuristic approach to define the embedding sizes. In language modelling, it is often the case to see embeddings of size 600 or more. In the case of categorical variables, the data is much simpler. So, it we will define the embedding size as the minimum between half the number of categories, or 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dims = [int(data_ee[col].nunique()) for col in categorical_features]\n",
    "emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set GPU\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model. I have searched online for heuristics on the number of layers, hidden layer sizes and dropouts to use. I have settled on these, as they are the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FeedForwardNN(emb_dims, lin_layer_sizes=[1000, 500], output_size=1, emb_dropout=0.001, lin_layer_dropouts=[0.001,0.01]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a different loss function\n",
    "def MAPELoss(output, target):\n",
    "    return torch.mean(torch.abs((target - output) / target))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_epochs = 5 # 10 was suggested by EE paper, but loss was going up\n",
    "criterion = MAPELoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "epoch_losses = []\n",
    "for epoch in range(no_of_epochs):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for y, cat_x in tqdm(dataloader,position=0):\n",
    "        cat_x = cat_x.to(device)\n",
    "        y  = y.to(device)\n",
    "        # Forward Pass\n",
    "        preds = model(cat_x)\n",
    "        loss = criterion(preds, y)\n",
    "        # Backward Pass and Optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()/len(cat_x)\n",
    "    #print(np.mean(losses))\n",
    "    print(running_loss)\n",
    "    epoch_losses.append(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the categorical columns with the respective embedding shapes\n",
    "list(zip(categorical_features, model.emb_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('weights'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Embedding matrix for DayOfWeek: \\n\\n{model.emb_layers[1].weight}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move embeddings from GPU and put on CPU\n",
    "cpu_embs = model.emb_layers.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cat_embeddings(dataframe, embeddings, categorical_features):\n",
    "    \"\"\" Map the entity embeddings to the label encoding. Replace label encoded data \n",
    "    with the entity embeddings. Deletes original label encoded data.\n",
    "    \n",
    "    Example of entity embedding dataframe:\n",
    "    Events has 20 unique labels with size of embedding 10\n",
    "    \n",
    "    | Events | Events_0  | Events_1   | Events_2  |    ...     | Events_10 |\n",
    "    |--------|-----------|------------|-----------|------------|-----------|\n",
    "    | 0      | 2.384089  | 4.924449   | 15.312108 |    ...     | 13.249166 |\n",
    "    | 1      | -4.169876 | -6.012678  | 4.244546  |    ...     | 10.307323 |\n",
    "    | ...    |    ...    |    ...     |    ...    |    ...     | 23.748465 |\n",
    "    | 20     | 5.596208  | -15.757868 | 10.706656 |    ...     | 7.573045  |\n",
    "    \n",
    "    Args\n",
    "        dataframe: pd.DataFrame, original dataframe\n",
    "        embeddings: ModuleList, list of embedding weights\n",
    "    Returns\n",
    "        dataframe: pd.DataFrame, updated dataframe \n",
    "    \"\"\"\n",
    "    for i, cat_var in enumerate(categorical_features):\n",
    "        \n",
    "        # Retreive respective label encoder and transform data\n",
    "        dataframe[cat_var] = label_encoders[cat_var].transform(dataframe[cat_var])\n",
    "        # Create a dataframe from respective embedding matrix\n",
    "        df = pd.DataFrame(embeddings[i].weight.detach().numpy())\n",
    "        # Rename columns to include the categorical name\n",
    "        df = df.add_prefix(f'{cat_var}_')\n",
    "        # Set a name for the index\n",
    "        df.index.name = cat_var\n",
    "        # Move named index as a column to join on\n",
    "        df.reset_index(inplace=True)\n",
    "        # Join original dataframe with new categorical embeddings\n",
    "        dataframe = dataframe.merge(df, how='left',on=cat_var)\n",
    "        # Remove original label encoded variable\n",
    "        _ = dataframe.pop(cat_var)\n",
    "                \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 003 - Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(fpath, fname_train, fname_test, seed, cpu_embs, categorical_features):\n",
    "    \"\"\"\n",
    "    Load data and preprocess it for training a model. This will also fill \n",
    "    missing values for continuous variables with the median and create a \n",
    "    flag column correspondingly. This isn't used for hyper-parameter tunning.\n",
    "    \n",
    "    Args\n",
    "        fpath: string, folder path \n",
    "        fname_train: string, name with parquet extention\n",
    "        fname_test: string, name with pkl extention\n",
    "        seed: int, \n",
    "    Return\n",
    "        dtrain: xgb.DMatrix, dataset used for training\n",
    "        dvalid: xgb.DMatrix, dataset used for early stopping\n",
    "        dtest: xgb.DMatrix, dataset used for submittion to kaggle\n",
    "    \"\"\"\n",
    "    # Define the features to load\n",
    "    columns = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', \n",
    "                'CompetitionMonthsOpen', 'Promo2Weeks', 'StoreType', 'Assortment', \n",
    "                'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear', \n",
    "                'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', \n",
    "                'StateHoliday_bw', 'SchoolHoliday_fw', 'SchoolHoliday_bw', 'CompetitionDistance', \n",
    "                'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC', 'Max_Humidity', \n",
    "                'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', 'Mean_Wind_SpeedKm_h', \n",
    "                'CloudCover', 'trend', 'trend_DE', 'AfterStateHoliday', 'BeforeStateHoliday', \n",
    "                'Promo', 'SchoolHoliday', 'Date', 'Sales']\n",
    "    \n",
    "    # 1) Load data\n",
    "    train = pd.read_parquet(Path(fpath,fname_train), columns=columns)\n",
    "    test = pd.read_pickle(Path(fpath,fname_test))\n",
    "    columns.remove('Sales')\n",
    "    test = test[columns + ['Id']]\n",
    "    \n",
    "    # 2) Let's use the date as the index and sort the data\n",
    "    train.sort_values('Date', inplace=True)\n",
    "    train.set_index('Date', inplace=True)\n",
    "    test.sort_values('Date', inplace=True)\n",
    "    test.set_index('Date', inplace=True)\n",
    "    columns.remove('Date')\n",
    "    test_ids = test.pop('Id') #Useful for submission\n",
    "    \n",
    "    # Deal with missing categorical values\n",
    "    train['PromoInterval'] = train['PromoInterval'].fillna('#NAN#')\n",
    "    train['Events'] = train['Events'].fillna('#NAN#')\n",
    "    test['PromoInterval'] = test['PromoInterval'].fillna('#NAN#')\n",
    "    test['Events'] = test['Events'].fillna('#NAN#')    \n",
    "\n",
    "    # 3) Deal with missing continuous values\n",
    "    for col_name in ['CompetitionDistance', 'CloudCover']:\n",
    "        # Add na cols\n",
    "        train[col_name+'_na'] = pd.isnull(train[col_name])\n",
    "        test[col_name+'_na'] = pd.isnull(test[col_name])\n",
    "        # Fill missing with median (default in FastAI)\n",
    "        fillter = train[col_name].median()\n",
    "        train[col_name] =  train[col_name].fillna(fillter)\n",
    "        test[col_name] =  test[col_name].fillna(fillter)\n",
    "        columns.append(col_name+'_na')\n",
    "        \n",
    "    # Replace categorical variables with embeddings\n",
    "    train = add_cat_embeddings(train, cpu_embs, categorical_features)\n",
    "    test = add_cat_embeddings(test, cpu_embs, categorical_features)\n",
    "    columns = list(train.columns)\n",
    "\n",
    "    columns.remove('Sales')\n",
    "    # 4) Apply log transform to the target variable\n",
    "    train['Sales'] = np.log1p(train['Sales'])\n",
    "\n",
    "    # 5) Set asside a random 1% sample for early stopping\n",
    "    # I'm separating the X and y simply for ease of use for the target encoder\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(train[columns], train['Sales'], test_size=0.01, random_state=seed)\n",
    "\n",
    "    # 6) Deal with categorical variables\n",
    "    te = TargetEncoder(handle_missing='value')\n",
    "    train_X = te.fit_transform(train_X, cols=['StoreType', 'Assortment', 'PromoInterval', 'State', 'Events'], y=train_y)\n",
    "    valid_X = te.transform(valid_X)\n",
    "    test = te.transform(test)\n",
    "\n",
    "    # 7) Convert to DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(train_X, train_y)\n",
    "    dvalid = xgb.DMatrix(valid_X, valid_y)\n",
    "    dtest = xgb.DMatrix(test)\n",
    "    \n",
    "    return dtrain, dvalid, dtest, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "dtrain, dvalid, dtest, test_ids = data_pipeline('../data/03_primary', 'clean_train_valid.parquet', 'test_clean.pkl', seed, cpu_embs, categorical_features)\n",
    "\n",
    "params = {'tree_method':'gpu_hist',\n",
    "          'objective': 'reg:squarederror',\n",
    "          'seed':seed}\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "model = xgb.train(params=params, dtrain=dtrain, num_boost_round=4000, early_stopping_rounds=20, feval=rmspe_xg, verbose_eval=200, evals=watchlist)\n",
    "\n",
    "predictions = model.predict(dtest)\n",
    "\n",
    "pd.DataFrame({'Id':test_ids,\n",
    "              'Sales':np.exp(predictions)}).to_csv('../data/03_primary/exp_003.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Results\n",
    "    \n",
    "Private Score: 0.15251  \n",
    "Public Score: 0.14079\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 004 - Hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "all_cols = ['Store', 'DayOfWeek', 'Year', 'Month', 'Day', 'StateHoliday', 'CompetitionMonthsOpen',\n",
    "            'Promo2Weeks', 'StoreType', 'Assortment', 'PromoInterval', 'CompetitionOpenSinceYear', 'Promo2SinceYear',\n",
    "            'State', 'Week', 'Events', 'Promo_fw', 'Promo_bw', 'StateHoliday_fw', 'StateHoliday_bw',\n",
    "            'SchoolHoliday_fw', 'SchoolHoliday_bw','CompetitionDistance', 'Max_TemperatureC', 'Mean_TemperatureC', 'Min_TemperatureC',\n",
    "            'Max_Humidity', 'Mean_Humidity', 'Min_Humidity', 'Max_Wind_SpeedKm_h', \n",
    "            'Mean_Wind_SpeedKm_h', 'CloudCover', 'trend', 'trend_DE',\n",
    "            'AfterStateHoliday', 'BeforeStateHoliday', 'Promo', 'SchoolHoliday', 'Sales','Date']\n",
    "\n",
    "data_train = pd.read_parquet('../data/03_primary/clean_train_valid.parquet', columns=all_cols)\n",
    "\n",
    "# Sort date-wize and set Date as index\n",
    "data_train.sort_values('Date',inplace=True)\n",
    "data_train.set_index('Date', inplace=True)\n",
    "\n",
    "# Deal with missing categorical values\n",
    "data_train['PromoInterval'] = data_train['PromoInterval'].fillna('#NAN#')\n",
    "data_train['Events'] = data_train['Events'].fillna('#NAN#')\n",
    "        \n",
    "# Replace categorical variables with embeddings\n",
    "data_train = add_cat_embeddings(data_train, cpu_embs, categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize():\n",
    "    space = {\n",
    "            # Learning rate: default 0.3 -> range: [0,3]\n",
    "           'eta': hp.quniform('eta', 0.01, 0.3, 0.001),\n",
    "            # Control complexity (control overfitting)\n",
    "            # Maximum depth of a tree: default 6 -> range: [0:∞]\n",
    "            'max_depth':  hp.choice('max_depth', np.arange(5, 10, dtype=int)),\n",
    "            # Minimum sum of instance weight (hessian) needed in a child: default 1\n",
    "            'min_child_weight': hp.quniform('min_child_weight', 1, 3, 1),\n",
    "            # Minimum loss reduction required: default 0 -> range: [0,∞]\n",
    "            'gamma': hp.quniform('gamma', 0, 5, 0.5),\n",
    "\n",
    "            # Add randomness to make training robust to noise (control overfitting)\n",
    "            # Subsample ratio of the training instance: default 1\n",
    "            'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "            # Subsample ratio of columns when constructing each tree: default 1\n",
    "            'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "            \n",
    "            # Regression problem\n",
    "            'objective': 'reg:squarederror',\n",
    "            # For reproducibility\n",
    "            'seed': seed,\n",
    "            # Faster computation\n",
    "            'tree_method':'gpu_hist'\n",
    "            }\n",
    "        \n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=100)\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_folds = 3\n",
    "train_times = []\n",
    "\n",
    "# Prepare data\n",
    "data_x = data_train.copy()\n",
    "data_y = data_x.pop('Sales')\n",
    "data_y = np.log1p(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score(params):\n",
    "    # Initialize timer\n",
    "    start_time = time()\n",
    "    # Initialize list of scores for each fold\n",
    "    score_list = []\n",
    "    # Set to 20 to have validation sets of about the same size as the real test size\n",
    "    tscv = TimeSeriesSplit(n_splits=20)\n",
    "    # Initialize a split counter\n",
    "    split_iteration = -1\n",
    "     \n",
    "    for train_index, test_index in tscv.split(data_x):\n",
    "        \n",
    "        # Select the folds from the end (superintended) for the desired number of splits.\n",
    "        split_iteration+=1\n",
    "        if split_iteration < 20 - pred_folds: continue\n",
    "\n",
    "        # select 1% of the training data for early stopping\n",
    "        train_index, es_index = train_test_split(train_index, test_size=0.01, random_state=seed)\n",
    "        \n",
    "        # Select data by index from the time series cross validatation split\n",
    "        X_train, X_val, X_test = data_x.iloc[train_index].copy(), data_x.iloc[es_index].copy(), data_x.iloc[test_index].copy()\n",
    "        y_train, y_val, y_test = data_y.iloc[train_index].copy(), data_y.iloc[es_index].copy(), data_y.iloc[test_index].copy()\n",
    "        \n",
    "        # Deal with missing continuous values\n",
    "        for col_name in ['CompetitionDistance', 'CloudCover']:\n",
    "            # Add na cols\n",
    "            X_train[col_name+'_na'] = pd.isnull(X_train[col_name])\n",
    "            X_val[col_name+'_na'] = pd.isnull(X_val[col_name])\n",
    "            X_test[col_name+'_na'] = pd.isnull(X_test[col_name])\n",
    "\n",
    "            # Fill missing with median (default in FastAI)\n",
    "            fillter = X_train[col_name].median()\n",
    "            X_train[col_name] =  X_train[col_name].fillna(fillter)\n",
    "            X_val[col_name] =  X_val[col_name].fillna(fillter)\n",
    "            X_test[col_name] =  X_test[col_name].fillna(fillter)\n",
    "            \n",
    "        # Dmatrix for optimization - structure de donnees optmisizer\n",
    "        dtrain = xgb.DMatrix(X_train, y_train)\n",
    "        dtest = xgb.DMatrix(X_test, y_test)\n",
    "        dvalid = xgb.DMatrix(X_val, y_val)\n",
    "        \n",
    "        # the last one is used for early stopping - contraire de doc\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "            \n",
    "        # Can use feval for a custom objective function\n",
    "        model = xgb.train(params, dtrain, early_stopping_rounds=100, num_boost_round=4000, verbose_eval=0, feval=rmspe_xg, evals=watchlist) \n",
    "\n",
    "        # validation - this will be the score that we append to a list. which will be fed as the score? Is all of this the score?\n",
    "        y_pred = model.predict(xgb.DMatrix(X_test))\n",
    "        #error = mean_absolute_error(y_test, y_pred)\n",
    "        error = rmspe(y_test, y_pred)\n",
    "        score_list.append(error)\n",
    "    #print(f'Took  {np.round(time()-start_time,0)} (s) - RMSPE score: {np.mean(score_list)} - PARAMS: {params}')\n",
    "    train_times.append(np.round(time()-start_time,0))\n",
    "    return np.mean(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# trials will contain logging information\n",
    "trials = Trials()\n",
    "\n",
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = pd.DataFrame()\n",
    "\n",
    "for i in range(len(trials.trials)):\n",
    "\n",
    "    row = pd.concat([pd.DataFrame({'loss':[trials.trials[i]['result']['loss']]}), \\\n",
    "                     pd.DataFrame(trials.trials[i]['misc']['vals'])], axis=1)\n",
    "    \n",
    "    summary_table = summary_table.append(row)\n",
    "\n",
    "summary_table = pd.concat([pd.DataFrame({'exp_time':train_times}),summary_table.reset_index(drop=True)],axis=1)\n",
    "summary_table = summary_table.sort_values('loss')\n",
    "summary_table.to_pickle('trials_004.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table.sort_values('loss').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use hyperparameters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "dtrain, dvalid, dtest, test_ids = data_pipeline('../data/03_primary', 'clean_train_valid.parquet', 'test_clean.pkl', seed, cpu_embs, categorical_features)\n",
    "\n",
    "# Retreive the best parameters using space_eval. This is because hyperopt returns the index of the value used when the distribution\n",
    "# was set to hp.choice(...)\n",
    "best_hyperparams = space_eval(space, best_hyperparams)\n",
    "\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "training_curves_004 = {}\n",
    "model = xgb.train(params=best_hyperparams, dtrain=dtrain, num_boost_round=4000, early_stopping_rounds=100, feval=rmspe_xg, verbose_eval=200, evals=watchlist,\n",
    "                                           evals_result=training_curves_004)\n",
    "\n",
    "predictions = model.predict(dtest)\n",
    "\n",
    "pd.DataFrame({'Id':test_ids,\n",
    "              'Sales':np.exp(predictions)}).to_csv('../data/03_primary/exp_004.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_curves_004['train']['rmspe'],label='Train')\n",
    "plt.plot(training_curves_004['eval']['rmspe'],label='Eval')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('RMSPE')\n",
    "plt.title('RMSPE Loss with early stopping = 100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Results\n",
    "\n",
    "Private Score: 0.13081  \n",
    "Public Score: 0.11572\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:supertab-xg] *",
   "language": "python",
   "name": "conda-env-supertab-xg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
